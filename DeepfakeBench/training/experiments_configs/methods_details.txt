I did a little bit of research to give you a better background. Here it is:
----------------------------------------------------------------------------------
Deepfake Generation Methods: DF40, FaceForensics++, Google Veo3, and HeyGen
FaceForensics++ (2019) – Early Deepfake Methods

FaceForensics++ is a benchmark dataset released in 2019 that contains four representative facial manipulation methods
ar5iv.labs.arxiv.org
. These techniques, popular around 2016–2019, include two computer-graphics approaches and two learning-based deepfakes
ar5iv.labs.arxiv.org
. Each method’s origin and properties are summarized below:

Face2Face (2016) – 3D Facial Re-enactment. This is a graphics-based method by Thies et al. (CVPR 2016) that transfers the facial expressions of a source actor to a target actor in real time
ar5iv.labs.arxiv.org
. It builds a 3D face model of the target and warps it to mimic the source’s expressions, without changing the target’s identity. Human detectability: Because Face2Face only alters expressions (a local change) without introducing obvious new elements, the resulting fake is subtle. In a user study, humans struggled to spot Face2Face fakes – these were “particularly difficult to detect” since they don’t introduce big semantic changes
ar5iv.labs.arxiv.org
. AI detectability: Automated detectors can catch Face2Face by learning its minor rendering artifacts (e.g. slight skin tone or motion inconsistencies). However, under heavy compression or low quality, even some detectors drop to near human-level accuracy
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Overall, Face2Face fakes are hard for humans but easier for AI (given high-quality input) because the method’s warping and rendering leave subtle pixel-level cues.

FaceSwap (traditional) – Classic Face Replacement. FaceSwap refers to a class of graphics-based face-swapping techniques (one implementation was an open-source 2018 tool
ar5iv.labs.arxiv.org
). It replaces one person’s face with another’s by aligning facial landmarks and blending the face region onto the target. This method does not use deep learning and was an early approach to identity swap. Human detectability: FaceSwap fakes often exhibit global mismatches – e.g. blending seams, lighting inconsistencies, or a “mask-like” look around the face edges – which alert a keen viewer. If the viewer knows the persons involved, the identity discrepancy is also a giveaway. Thus, FaceSwap outputs can be noticeable to humans (especially when artifacts like color mismatch or stiff edges occur). AI detectability: These fakes are relatively easier for algorithms to detect. They involve a blending process that leaves telltale artifacts; indeed, detectors focused on detecting blending have achieved state-of-the-art results on older face-swapped data
arxiv.org
. Many FaceSwap forgeries share common low-level artifacts (blurred boundaries, etc.), so a CNN-based detector can learn to spot them reliably. Overall, FaceSwap is one of the easier methods for both humans and AI to flag, due to its visible seams and simpler generation technique (particularly in high-resolution frames).

“DeepFakes” (2017–2018) – Deep Learning Face Swap (Autoencoder-Based). The term DeepFakes originally comes from a 2017 Reddit user’s AI face-swapping technique, and here it denotes a specific deep learning method (popularized by tools like FakeApp and the “deepfakes/faceswap” GitHub)
ar5iv.labs.arxiv.org
. This approach trains an auto-encoder with a shared encoder and two decoders – one for each face – to swap one person’s face onto another
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Each pair of source–target faces requires a dedicated training (which was time-consuming), and the output face is blended into the video frames with a post-processing pipeline
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Architecture: It’s a neural network (often a CNN autoencoder) that learns to reconstruct Face A and Face B; by feeding Face A through Face B’s decoder, it generates a face swap. Human detectability: Early DeepFakes had moderate visual quality – often slightly blurry or with imperfect alignment. Humans could sometimes notice odd details (glitches in facial movements, slight blurriness, or “plastic” skin texture) and especially edges where the swapped face meets the background. However, in many cases these fakes fooled human observers, particularly if the video was not high-resolution. (In fact, human accuracy on raw DeepFake videos was only around 68–78% in FF++ tests
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
, meaning many people were guessing.) AI detectability: Because DeepFakes share a common generation pipeline, they tend to leave consistent artifacts. For example, the autoencoder’s limited resolution and the blending step introduce unnatural color/texture patterns that a neural detector can learn
ar5iv.labs.arxiv.org
. Indeed, the FaceForensics++ study showed that a deep CNN (XceptionNet) could detect these fakes with very high accuracy – over 98% on raw videos, far outperforming humans
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Modern detectors even exploited specific quirks (like odd eye blinking frequencies or face color inconsistencies
ar5iv.labs.arxiv.org
). Thus, DeepFakes are moderately challenging for humans but were relatively easy for specialized AI detectors by 2019, since the method’s neural synthesis left telltale signals that algorithms learned to recognize.

NeuralTextures (2019) – Neural Rendering for Re-enactment. NeuralTextures (Thies et al. 2019) is a learning-based approach that re-enacts a person’s face using a neural rendering engine
ar5iv.labs.arxiv.org
. It records a person’s appearance as a neural texture map, then synthesizes new expressions by feeding the texture through a rendering network. The result is a highly realistic facial animation of the same person (identity preserved, expressions changed). Human detectability: NeuralTextures outputs were very realistic for their time – they changed only subtle facial motions. In the FaceForensics++ human study, NeuralTextures fakes were actually the hardest for people to identify: human accuracy on NT was below random chance (under 50%) on high-quality images
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Observers found almost no obvious flaws, as the identity and overall look remained consistent; only slight “unnatural” texture changes or temporal flicker might give it away. AI detectability: NeuralTextures does produce some high-frequency artifacts in the synthesized regions (the “neural texture” may introduce tiny pixel-level inconsistencies). A well-trained AI model can pick up on these subtle cues. For instance, an Xception model trained on FF++ data detects NeuralTextures fakes with high precision under favorable conditions (the FF++ paper noted near-perfect detection on uncompressed NT data by their CNN)
ar5iv.labs.arxiv.org
. But under compression/noise, detectors struggle more on NT than on simpler swaps. Overall, NeuralTextures fakes are very deceptive to humans (virtually undetectable without tools
ar5iv.labs.arxiv.org
) and challenging for AI detectors too if the detector isn’t trained on similar data – though given training, AI can achieve strong accuracy.

FaceForensics++ summary: These four methods were the “first generation” of deepfakes in public benchmarks (circa 2016–2019). They each have distinct artifacts: FaceSwap/DeepFakes often cause global blending issues, whereas Face2Face/NeuralTextures cause more local subtle changes. Humans generally found identity swaps (DeepFakes, FaceSwap) a bit easier to notice than expression swaps – the FF++ study reported that expression-only fakes (Face2Face, NeuralTextures) fooled humans more often because they “introduce only subtle visual artifacts”
ar5iv.labs.arxiv.org
. In fact, NeuralTextures was so hard that people did worse than random guessing on it
ar5iv.labs.arxiv.org
. Meanwhile, state-of-the-art AI detectors (circa 2019) could learn each method’s footprint and achieved over 90% detection accuracy on these fakes in controlled settings
ar5iv.labs.arxiv.org
. However, those detectors were narrow in scope – trained on these specific techniques. As we’ll see next, newer deepfake methods have emerged that reduce these obvious artifacts, making detection more difficult.

DF40 (2024) – Diverse and Modern Deepfake Methods

DF40 is a large-scale benchmark introduced in mid-2024 to represent “next-generation” deepfakes
huggingface.co
huggingface.co
. It includes 40 distinct generation techniques – ten times more diverse than FF++ – spanning the period from 2016 up to 2024’s latest AI models
arxiv.org
arxiv.org
. Crucially, DF40 covers four broad categories of facial forgeries
arxiv.org
:

Face-Swapping (FS) – Identity swaps between two people. DF40 implements numerous face-swap methods beyond the old DeepFakes/FaceSwap. This includes recent methods that no longer rely on naive blending. For example, SimSwap (CVPR 2020) and FaceDancer (2023) use an identity+background encoder to generate the entire output image (face and background together)
arxiv.org
. These “next-gen” swaps avoid the obvious seams of older approaches – they directly render a new image of Person A in Person B’s pose, rather than pasting one onto the other
arxiv.org
. DF40 also features FSGAN (2019), an improved GAN-based swap that first reenacts the target’s pose before swapping
arxiv.org
, and DeepFaceLab (2018–2020), the popular open-source tool that many real-world deepfakes are made with. (DeepFaceLab is similar to the original DeepFakes but highly optimized; it requires training one model per face-pair and produces high-quality swaps with proper tuning
arxiv.org
.) Another example is FaceShifter (2020), a state-of-the-art swap method that was used to create the high-quality Celeb-DF v2 dataset
arxiv.org
. Detectability: The introduction of these advanced FS methods makes detection tougher. Human perspective: When done well, modern face swaps can be nearly seamless – no border artifacts – so a viewer might not suspect anything unless the impersonation is implausible or the face has subtle uncanny aspects. Older swaps with blending could often be spotted by a sharp eye noticing color tone mismatches; newer ones remove that clue. AI perspective: Traditional detectors that “look for the blending” will fail on non-blended fakes like SimSwap
arxiv.org
. DF40’s authors note that many prior detectors assumed the presence of a compositing boundary – an assumption broken by these new methods
arxiv.org
. Thus, detection models must shift to other signals (like identity mismatches or fine-scale artifacts). Some global cues remain – e.g. the swapped face may not perfectly match the target’s head geometry or lighting – but these are subtle. In DF40 evaluations, standard detectors struggled: for instance, Xception’s accuracy on high-quality FaceDancer and SimSwap fakes is much lower than on older blended fakes
arxiv.org
. Overall, face-swapping has evolved – modern techniques produce highly realistic identity swaps that are harder for both humans and AI to spot, compared to the obvious fakes of a few years ago
arxiv.org
.

Face-Reenactment (FR) – Driving a target face’s expressions/motion using a source. DF40 contains 13 different reenactment methods
arxiv.org
, reflecting rapid advances in this area. It includes the two classic ones from FF++ (Face2Face 2016 and NeuralTextures 2019) for completeness
arxiv.org
, but also many new techniques: e.g. FOMM (First Order Motion Model, NeurIPS 2019) – a one-shot model that can animate a single image using a driving video
arxiv.org
; Wav2Lip (2020) – which generates realistic lip movements in a video given an audio track
arxiv.org
; TPSM (2020) and DaGAN (2020) – advanced pose-transfer and audio-driven models; Marionette (MRAA), MotionCoGAN (MCNet), LIA (Live Image Animation), OneShot head reenactment, PiRenderer, SadTalker (2023) – a recent method for making a portrait “talk” with audio, and others
arxiv.org
arxiv.org
. Some methods are video-driven (requiring a driving video or landmarks to mimic), others are audio-driven (synchronize speech). Detectability: For humans, reenactment fakes can be very insidious because the person’s identity stays the same – only their expression or speech content is altered. If the model is good, the fake video looks exactly like the real person just behaving differently. As noted earlier, even older reenactment like Face2Face often fooled viewers since the changes are subtle (no big “face transplant” to cause alarm)
ar5iv.labs.arxiv.org
. Newer models (e.g. Wav2Lip or SadTalker) have improved lip-sync and facial motion quality to the point that casual observers may not notice anything amiss. Humans might only detect these fakes if there’s a glitch (e.g. briefly odd mouth shapes, asynchronous audio) or if the situation is implausible. For AI, detecting reenactments often requires focusing on local temporal inconsistencies – e.g. lip movements that don’t perfectly match the audio (for audio-driven fakes) or slight warping artifacts around the mouth/eyes. Some research has proposed cross-modal checks (comparing the speech content to the person’s mouth movements, to catch audio-driven fakes). DF40’s variety shows that many FR methods produce realistic, low-artifact outputs – detectors trained on older data may underperform on these. Indeed, DF40 results show that no single detector dominated, and some strong detectors struggled on particular FR fakes (performances vary widely per method)
arxiv.org
arxiv.org
. In sum, **face reenactment fakes are generally hard for humans to notice (especially when only expressions or lip movements change), and challenging for AI unless the model specifically learns the nuanced artifacts or audio-visual mismatches each method might introduce.

Entire Face Synthesis (EFS) – Generating a face from scratch (no specific source person). This category corresponds to “face AI-generated content” – realistic human faces created by generative models (usually as still images)
arxiv.org
arxiv.org
. DF40 includes both GAN-based and diffusion-based face generators. For example: StyleGAN2 (2019) & StyleGAN3 (2021) – Nvidia’s GAN models famous for producing high-fidelity fake faces; VQGAN (2021) – a generative model that uses vector quantization
arxiv.org
; Stable Diffusion (2022) and MidJourney v5/v6 (2022–2023) – latent diffusion models that can produce photorealistic human faces from text prompts
arxiv.org
arxiv.org
; and even the WhichFaceIsReal dataset images (which were StyleGAN-based fake faces used in an online game)
arxiv.org
. These entire-face fakes do not involve swapping or editing a specific real person – they produce a completely synthetic person’s face (often indistinguishable from a real one). Detectability: Human perspective: Modern face synthesis models have achieved stunning realism. By 2023, StyleGAN and diffusion-generated portraits are often indistinguishable from real photos at a glance. Humans might catch small oddities if they scrutinize – older StyleGAN faces sometimes had telltale artifacts like asymmetrical or “shimmering” eyes and teeth, or subtle background distortions. But current diffusion models (MidJourney, SD) have largely overcome those flaws for faces. In fact, a high-quality AI-generated face can fool even experts unless there’s a specific flaw (like surreal context or the infamous difficulty in generating hands/ears, which is less an issue for a single face). AI perspective: Traditional fake detectors (pre-2022) were often trained on GAN artifacts – and indeed, methods were developed to spot neuron-level or frequency-level patterns left by GAN generation. But diffusion models produce more natural noise statistics, reducing obvious artifacts
arxiv.org
. DF40 highlights that detectors trained on older “deepfake” data (face swaps) might not generalize to these pure AI-synthesized faces, since those detectors looked for the wrong cues
arxiv.org
. Specialized detectors for generative images (e.g. ones used in the GenImage benchmark) can achieve good results by picking up subtle patterns or by using metadata/watermarks when available
arxiv.org
. Overall, entirely synthetic faces are very hard for humans to recognize as fake, and AI detectors require up-to-date training (on the latest generators) to reliably catch them. Notably, as generative models improve, researchers increasingly rely on intrinsic watermarks or model fingerprints for detection, since visual artifacts are minimal.

Face Editing (FE) – Altering or attribute editing of a real face. This involves taking a real image and changing some aspect: e.g. changing the person’s age, adding a smile, changing hair color, or even changing their identity in a subtle way. DF40 includes editing methods like StarGAN (2018) and StarGAN v2 (2020) – GANs that translate face attributes (e.g. turning a face from male to female, or adding facial hair)
arxiv.org
; StyleCLIP (2021) – which uses CLIP to guide StyleGAN in editing an image via text prompts
arxiv.org
; and e4e (Encoding for Editing, 2021) – which embeds an image into StyleGAN’s latent space and modifies it for edits
arxiv.org
. Also included is CollabDiff (2023) – a diffusion-based face editor that can apply changes like expressions or artistic styles
arxiv.org
. Detectability: Human perspective: High-quality face edits can be very natural-looking. For example, an AI might take a photo of someone frowning and output the same photo with a smile – to a human, it just looks like a normal photo of that person smiling. Unless the edit is poorly done (e.g. blurriness around the mouth or inconsistent lighting), a viewer would have no reason to suspect it’s fake. Many edits are local and preserve most of the image, so there are few global cues. AI perspective: Detecting edited images is notoriously difficult, because the manipulation can be minor. There may be subtle inconsistencies in texture where changes were made (for instance, StarGAN may leave slight artifacts in the altered regions). Detectors might need to focus on statistical differences in the edited area versus the rest of the image. If an editing method leaves a signature (say a certain noise pattern or smoothing), an AI can learn it, but in general, edited images approach the quality of genuine photos. Indeed, DF40’s breadth shows that a detector needs to handle both obvious fakes and tiny edits. In their evaluations, some detectors overfit to identity swaps and performed poorly on edited images, which don’t have the same kind of artifacts
arxiv.org
arxiv.org
. In summary, face edits that are slight (like expression or minor attribute changes) are almost invisible to the human eye, and AI detectors struggle unless specifically trained for those subtle forgeries.

Overall DF40 insights: The DF40 dataset was created because existing detectors were overly tuned to a narrow set of forgery types. Most previous benchmarks had only face swaps (often with blending) and maybe a couple of reenactment methods
arxiv.org
. By contrast, DF40 brings in diffusion-generated faces, high-fidelity reenactments, one-shot talking head generators, and commercial deepfake tools – reflecting what’s “in the wild” by 2024. The authors found that detectors trained just on older data (like FF++) fail to generalize to these “nowadays’ SOTA deepfakes”
huggingface.co
. For example, a top detector might perform well on FaceSwap or basic DeepFakes, but then plummet in accuracy on a novel method like an AI-generated face or a HeyGen video. Indeed, among 8 tested detection methods, none had a clear advantage on all types; many state-of-the-art detectors lost their edge when faced with DF40’s diversity
arxiv.org
. A notable finding was that a vision-language model (CLIP) performed surprisingly well as a general detector – possibly because its large-scale training taught it more universal features of real vs fake faces
arxiv.org
. This suggests that broad, robust features might be needed to catch the latest fakes, rather than hyper-specialized artifact checks. From a human perspective, DF40 confirms what many fear: today’s best deepfakes are extremely convincing. As generation quality approaches photorealism (e.g. high-resolution faces with correct lighting and synchronized speech), average people have almost no chance to discern fakes by sight alone
jumio.com
news.aibase.com
. In online discussions about these new tools, “the line between reality and AI is [seen as] blurred”
news.aibase.com
.

For curriculum design, DF40’s range implies grouping methods by similarity: e.g. detectors might first tackle simpler swaps with obvious global artifacts, then progress to subtle expression manipulations, and finally to entirely synthetic or high-fidelity fakes that demand fine-grained analysis. The dataset also underlines the need to train on many types of forgeries to build a detector that can handle whatever emerges next
huggingface.co
huggingface.co
.

Google Veo 3 (2025) – Next-Gen Multimodal Deepfake Model

Veo 3 is Google DeepMind’s latest AI video generation model, unveiled at Google I/O in 2025
jumio.com
. It represents a new level of deepfake capability: given a text prompt (and optional image input), Veo3 can produce a fully animated, high-resolution video with realistic visuals and synchronized audio. It is essentially a text-to-video system for generating short films or clips. Here are key points about Veo 3:

Date & Context: Announced in May 2025, Veo 3 is the successor to Google’s earlier video models (Veo 1 and 2). It was presented as a milestone in AI, widely publicized in mid-2025 as pushing the frontier of generative video
news.aibase.com
. For example, after the I/O demo, DeepMind’s web traffic spiked, reflecting massive interest in its capabilities
news.aibase.com
. This model is part of Google’s AI ecosystem (available via Google’s Gemini/AI Studio platforms
deepmind.google
).

Architecture: Veo 3 uses a latent diffusion transformer approach for video. In simpler terms, it extends the diffusion models (famous in image generation like Stable Diffusion) into the time dimension. Technically, it’s a 3D latent diffusion model: it operates on a 3D tensor (height × width × time) and generates video frames by iterative denoising in a latent space
medium.com
medium.com
. Audio is also generated in tandem, fused in the model’s latent representation
medium.com
storage.googleapis.com
. A high-level description from DeepMind’s tech report: “Veo 3 uses latent diffusion, jointly on temporal audio latents and spatio-temporal video latents, with a transformer-based denoising network”
storage.googleapis.com
. In practical terms, Veo3’s backbone is like Stable Diffusion’s U-Net, but upgraded to handle sequences of frames and sound (it uses 3D convolutions and a time-aware noise schedule)
medium.com
medium.com
. This allows it to maintain coherence frame-to-frame (objects keep their appearance and motion is smooth) and to sync audio events with the visuals (e.g. lip movements matching dialogue)
medium.com
medium.com
. Veo 3 outputs up to 4K-resolution video with impressive physical realism – the model learned “real world physics” (e.g. plausible motion, lighting, gravity) in scenes
deepmind.google
. Essentially, it’s one of the most advanced deepfake/generative models to date, blending cutting-edge diffusion and transformer techniques for video.

Capabilities: This model can generate complex scenes: e.g. the demo showed “an old sailor facing the sea, with the sound of waves and spoken dialogue, all with stunning realism”
news.aibase.com
news.aibase.com
. Veo 3 excels at lip-synced talking characters, dynamic camera angles, and consistent storylines over a clip. It also natively produces ambient sound and character voices to match the visuals
news.aibase.com
deepmind.google
. Compared to earlier generative videos, Veo3 has eliminated many common flaws – such as jittery frames, inconsistent objects, or silent unrealistic footage. Google specifically notes it “almost eliminates the flaws commonly found in traditional AI-generated content”
news.aibase.com
. This includes fixing temporal flicker and ensuring characters behave physically correctly (e.g. footsteps match ground contact). In short, Veo3 can create a deepfake video that looks like a professionally filmed and dubbed piece of content.

Human Detectability: Given the above, Veo3’s outputs are extremely hard for humans to distinguish from real videos. Observers have commented that it “raises the bar” and makes it “more and more challenging to differentiate reality versus fake content”
twitter.com
. The model’s coherence and high detail mean there are few obvious visual tells. In fact, Veo3 triggered widespread concern about deepfakes: the content it produces can fool everyone at first glance, blurring reality
news.aibase.com
. For example, a realistic deepfake of a public figure speaking with perfectly synced audio could be indistinguishable from a real recording, especially to an untrained eye. Google’s own op-ed notes “the quality of fake content is becoming harder to detect with the naked eye”
jumio.com
 – a direct reference to the threat posed by Veo3-level fakes. In response, Google has integrated a precaution: every video frame from Veo3 contains an invisible SynthID watermark that can be detected by a specialized tool
news.aibase.com
. (This is a passive watermark that doesn’t alter appearance but embeds a signal so that Google’s systems can later flag the video as AI-generated.) Of course, humans cannot see this watermark; without tools, a Veo3 video could be virtually impossible to call out as fake based solely on appearance.

AI Detectability: From an AI detector’s perspective, Veo3 output is at the cutting edge of difficulty. Conventional deepfake detectors rely on finding anomalies or artifacts – but Veo3 was designed to minimize these. The frames are high-quality and follow natural image statistics (thanks to diffusion). Motion is consistent, so there aren’t obvious discontinuities to exploit. The presence of SynthID is a game-changer, though: if a detector has access to the watermark key, it can immediately identify Veo3 content with near certainty. (This is more like an authentication check than a traditional vision analysis.) Absent the watermark, detectors would need to rely on subtle clues in the data distribution. As Veo3 is very new, there aren’t published studies of detectors on it yet, but one can extrapolate: a detector not specifically trained for Veo3 might perform poorly. For instance, a classic CNN detector might only achieve chance-level accuracy on these videos because they don’t exhibit the obvious artifacts it was trained on. However, if detectors use large models or multi-modal analysis, they might catch subtle issues (perhaps tiny errors in lip-sync or inconsistencies in fine texture under close inspection). Google’s stance is that automated detection should lean on embedded watermarks and “proactive security” for such advanced fakes
jumio.com
, since visual-based detection becomes uncertain. In summary, Veo3 content is extremely challenging for AI models to detect via traditional means, essentially requiring new detection approaches (like watermark checking or sophisticated model-fingerprinting). It represents the kind of deepfake that flips the advantage: without extra tools, even machines struggle, not just human eyes.

Significance for curriculum: Veo3 exemplifies the latest generation of deepfake technology – using powerful generative AI to produce wholly fabricated yet convincing videos. It shares some properties with diffusion-based image fakes (high photorealism) and with advanced reenactment (creating speech/lip-sync), but on a much larger scale. Any deepfake detection curriculum should treat Veo3 and similar models as the “final boss” – the hardest category that combines global realism (correct physics, lighting, etc.) and local realism (no obvious pixel artifacts). Early curriculum stages might focus on simpler artifacts, but ultimately one must prepare for content like Veo3’s, which might require out-of-band detection strategies (like verifying digital watermarks or source provenance) rather than purely analyzing pixels. As commentators have warned, models like Veo3 mean “deepfakes are here, and they are only getting harder to spot”
jumio.com
.

HeyGen (2020–2023) – Commercial Deepfake Video Platform

HeyGen is a popular commercial AI video generation platform (formerly known as Movio, rebranded to HeyGen in 2022–2023)
businessabc.net
. Unlike the research models above, HeyGen is an end-to-end service that allows users to create deepfake-style videos for practical purposes (marketing, e-learning, content creation) without technical expertise. It combines multiple AI technologies (text-to-speech, face animation, face swapping) behind a user-friendly interface
arxiv.org
arxiv.org
. Key details about HeyGen:

Timeline: The company was founded in 2020 (Los Angeles) under the name Movio, and later rebranded as “HeyGen” around 2022–2023
businessabc.net
. It saw rapid growth through 2023–2024, introducing many new features. By 2023, HeyGen could generate videos with over 300 customizable avatars and support 40+ languages via text-to-speech
heygen.com
. As of 2025 it has tens of thousands of customers and significant funding, indicating its maturity as a product
businessabc.net
businessabc.net
.

Method and Architecture: HeyGen is essentially a pipeline of AI models packaged as a service. Users can either choose a preset digital avatar or even create a custom one (by providing footage of a person), then input a script. HeyGen’s system will produce a video of that avatar speaking the script, in the chosen language, with synchronized voice and mouth movements. Under the hood, HeyGen likely uses a combination of techniques:

Talking-head generation: It animates a still image of the avatar to lip-sync the speech. This is akin to models like Wav2Lip or SadTalker – given an audio track, generate realistic mouth and facial movements on the person. The platform introduced “Talking Photo Generation” to animate photos with lifelike expressions
heygen.com
, which suggests a deepfake model for one-shot talking heads.

Text-to-Speech (TTS): HeyGen has an AI voice engine to convert the script into spoken dialogue (with options for different voices/accents). This is then used to drive the mouth movements. They highlight an “AI-powered text-to-speech functionality” as a key feature
arxiv.org
.

Face Swap/Face Morphing: In some modes, users can upload their own video and swap a face onto it. HeyGen even advertised “Multi-Face FaceSwapping” (swapping faces in group shots) in its Version 3.0 update (April 2023)
heygen.com
heygen.com
. This implies integration of a face swapping model (possibly similar to DeepFaceLab or FaceSwap, adapted for automation).

GAN/Diffusion Avatars: The avatars themselves might be either real filmed actors or AI-generated faces. HeyGen’s avatars mimic human expressions and can be customized, which suggests they may use GAN-based digital humans. For instance, they could use StyleGAN to create a new face and then a puppet model to animate it.

Video post-processing: The platform likely applies smoothing, blending, and editing to make sure the final video looks polished (e.g., matching the avatar into various backgrounds, adding subtitled text, etc., which are more on the software side).

In summary, HeyGen doesn’t publicize a single “model architecture”; it’s a suite of deepfake techniques integrated into a production pipeline. It leverages state-of-the-art research (from 2018–2022) on talking heads and face swaps, packaged with a slick UI.

Use Cases and Output Quality: HeyGen is used for synthetic presenters in videos – for example, a training video where a photorealistic avatar speaks the content, or marketing videos where an avatar delivers a message in multiple languages. The output is typically a person sitting or standing and talking to the camera (news-anchor style or spokesperson style). Because it’s a paid service with high-profile clients, the quality is quite high: the avatars have natural facial expressions and lip-sync that is nearly spot-on. Many users have tested the system and found it impressively realistic – the avatars blink, move slightly, and speak with emotion appropriate to the text (within limits). In one review, a tester noted the AI presenter video was so realistic that it could pass as a real human video on casual viewing (the differences were minor nuances in voice intonation and a slight uncanny valley in facial motion)
intelligenthq.com
youtube.com
. HeyGen also introduced features like video translation – you provide a real video of someone, and it outputs the same video but translated and with the speaker’s mouth altered to match the new language. This involves subtle deepfake work (re-synching lips to new audio) and has been demonstrated convincingly (e.g., making an English speaker appear to fluently speak Spanish, with correct lip movements).

Human Detectability: Videos made with HeyGen are designed to look professional. The avatars wear proper attire, have correct lighting, and the speech is clear. For an average person watching a HeyGen video (especially if they don’t know it’s AI), there may be no obvious signs of fakery. The movements and speech are not exaggerated or glitchy – they’re intentionally toned to look like a real person giving a scripted talk. Only on very close observation might one notice something slightly “off” (some users report that the facial expressions can appear a bit less dynamic or the voice, while very human-like, can have a telltale synthetic cadence). But generally, HeyGen’s content can fool viewers, particularly in short segments or if the viewer isn’t actively looking for flaws. It’s effectively a commercial-grade deepfake; part of its success is that many viewers don’t realize certain explainer videos they see are AI-generated.

AI Detectability: From a detector’s standpoint, HeyGen outputs pose a real challenge. These videos are highly realistic and diverse (since there are many avatar characters and various editing styles). In the DF40 benchmark, HeyGen was actually included as one of the “real-world” deepfake sources – and tellingly, even a strong CNN detector like Xception fared very poorly on it. In one experiment, Xception’s AUC on HeyGen-generated fakes was only ~0.39
arxiv.org
 – below random chance, indicating it was basically confounded by HeyGen’s realism. (For comparison, Xception scored 0.88 on DeepFaceLab fakes in that same test, showing it could catch the more traditional swaps, but HeyGen slipped through)
arxiv.org
. The DF40 authors explicitly note “HeyGen [and DeepFaceLab]... are widely used and highly realistic”, and because they require one-to-one pairing and lots of compute, only a small number of such fakes were in the dataset
arxiv.org
. This highlights that HeyGen’s fakes are among the most challenging for detectors. They likely have minimal artifacts: since HeyGen presumably uses high-quality face renders and smooth blending, there may be no obvious pixel discontinuities. Any artifacts might be method-specific (for instance, some analysts found that certain generation methods, including HeyGen, produce slight “radial” noise patterns on the face region
arxiv.org
, but detecting that requires fine analysis). HeyGen also claims to enforce some safety – possibly including its own deepfake detection or watermarks to prevent misuse
businessabc.net
 – but those would be internal. In practice, a general deepfake detector would need to be trained on HeyGen outputs or similar to reliably spot them.

Summary of HeyGen: It represents the commercialization of deepfake tech – making it easy for anyone to generate fake talking-head videos. Introduced in 2020 and evolving through 2023, it uses advanced architectures (GANs, diffusion, etc.) in a closed-source way. HeyGen fakes share properties with both reenactment (animating a fixed face) and face swaps (in case of using custom avatars or replacing a face in a user’s video). Many of its artifacts, if any, are very local and well-hidden (e.g. slightly imperfect lip corner movements). There are few global errors since the whole frame is produced consistently. Thus, HeyGen outputs are quite hard for both humans and AI to detect – they are polished enough for corporate use. For a training curriculum, this means content from platforms like HeyGen should be included at advanced stages, to ensure the detection model can handle real-world, high-quality deepfakes, not just academic examples. It also underscores the need for detectors to be exposed to non-public, proprietary fakes (which may not follow the same patterns as open-source algorithms). In real-world deployment, a detector that hasn’t seen examples from tools like HeyGen could be blindsided by them, as the DF40 results suggest.

Concluding Remarks – Building a Detection Curriculum

When designing a learning curriculum for a deepfake detection model, it’s crucial to consider the evolution of deepfake methods and their properties:

Older vs Newer Methods: Early deepfakes (circa 2016–2019, as in FaceForensics++) often had visible artifacts and simple attack patterns – they changed faces via cut-and-paste or low-res autoencoders, leaving global inconsistencies (e.g. mismatched face boundaries, unnatural skin tones). Detectors could start by learning these obvious cues. Newer methods (2020 onwards, as catalogued in DF40) have largely eliminated those easy giveaways. They produce fakes that are seamless at a global level, forcing detectors to hunt for local artifacts (like slight texture anomalies or inconsistencies in fine details or temporal dynamics). For example, an older face swap might be caught by a sharp boundary around the jawline, whereas a 2023 face swap (FaceDancer/SimSwap) will have no seam – a detector must instead recognize that the identity in the face doesn’t match the rest of the scene or catch subtle neural network noise in the image. Curriculum-wise, one might train the model first on older-generation fakes to grasp basic forensics (blending artifacts, etc.), then gradually introduce newer fakes that require more sophisticated feature learning (noise patterns, semantic inconsistencies). Keep in mind the warning from DF40: relying too much on outdated data can bias a detector to the wrong clues
huggingface.co
. So a good curriculum will continually incorporate newer deepfake styles to broaden the model’s knowledge.

Category differences: The detector should learn to handle different manipulation types: Face swaps vs. reenactments vs. fully synthetic faces vs. subtle attribute edits. These have differing difficulty for humans and AI. For instance, identity swaps (FS) change who the person is – a detector can leverage cues about identity mismatch (if it has face recognition capability, it might compare the face to an expected identity). Expression reenactment (FR) keeps identity same – so identity-based cues won’t work; instead, the detector might focus on motion inconsistencies or physiological signals (heart rate in face, blinking patterns) to detect unnaturalness. Entirely synthetic faces (EFS) may require the model to detect statistical anomalies in the image (since there is no “source” to compare, and the image might be perfectly coherent – detectors often use frequency analysis or check for oversmoothing in GAN images). Face edits (FE) are small changes – the detector might need to zoom in on the specific region (e.g., teeth, eyes) to catch doubling or blur. A curriculum can thus be organized by these categories, ensuring the model sees enough examples of each and learns the appropriate strategy for each kind. Notably, some detection approaches now use multi-task learning (e.g., training a network to classify the type of fake as well as detecting it), which implicitly forces learning these differences.

Global vs Local Artifacts: As mentioned, older deepfakes often had global artifacts (the entire face region looked off compared to the background, or the person looked like a different person – a big semantic change). New deepfakes often only have local artifacts (e.g., slightly odd eye texture, a few misaligned pixels at frame transitions, or subtle audio-visual mis-sync). A robust detector must transition from relying on global cues (which can be very discriminative but are not present in advanced fakes) to local, fine-grained cues. For example, an early lesson might be to detect a fake by noticing an abrupt edge around the face (global). A later lesson: detect a fake by noticing that the specular highlight in the eye is the same in both eyes (a known GAN quirk) or that a person’s heartbeat signal (tiny skin color pulsations) is absent or unnatural – these are very local/subtle clues. The curriculum should progressively lower the “signal-to-noise” ratio of artifacts: starting with fakes that practically wave a red flag, and ending with fakes that only a careful pixel-level or frequency-level analysis can discern.

Human vs AI difficulty: It’s insightful to note where humans excel or fail, because it often indicates what features a detector should focus on. Humans are good at noticing semantic anomalies (like “this person suddenly has a different face” or “the emotion doesn’t match the context”), but poor at noticing pixel-level artifacts or perfectly consistent lies (a well-done fake of a person doing something plausible will fool us). AI, conversely, can be trained to see pixel artifacts we miss, but might not understand high-level context. So, a comprehensive detection model might combine both approaches. For curriculum: early training can lean on the low-level artifacts (since those are concrete and label-able), but one should also incorporate training data that teaches the model some semantic context – e.g., multi-modal training where the model considers audio (to learn when speech and lips don’t match) or identity labels (to learn when a face does not match an claimed identity). In practice, modern detectors like those evaluated on DF40 are indeed incorporating larger “generalist” models (like CLIP) that have some semantic understanding, and these showed resilience by not overfitting to specifics
arxiv.org
.

In-the-wild challenges: Tools like HeyGen and advances like Veo3 remind us that deepfakes are not just academic experiments – they’re being deployed. Curriculum should include data from actual deepfake videos found in the wild (if available and labeled) or from these popular generation tools. For instance, including a few HeyGen-generated samples in validation could gauge if the detector is picking up on those. Likewise, anticipating the next Veo3-like model is important: encouraging the detector to learn watermark detection or to use passive signals (some generative models leave unique frequency footprints) could be part of advanced training. Google’s approach with SynthID suggests that future deepfakes may deliberately include watermarks for good actors, so a detector should leverage that when possible (perhaps a final stage in the curriculum is integrating a watermark checking module with the visual detector).

Finally, as deepfake methods continue to evolve (e.g. diffusion models are now state-of-the-art), the curriculum must be adaptive. Regular updates with newly emerged techniques (new GAN architectures, new audio-visual models, etc.) will be needed. The difference between a 2018 deepfake and a 2025 deepfake is enormous – as DF40 showed, a detector trained on 2018 techniques might fail catastrophically on 2024 fakes
huggingface.co
. Thus, the training program for a detection model should not consider deepfakes a static target. It should teach the model to generalize – possibly by focusing on fundamental properties of real media (e.g. physical consistency, biological signals) versus fake (e.g. subtle statistical noise differences), rather than just signature artifacts. Using a diverse dataset like DF40 in the curriculum is a good step toward this, exposing the model to 40 different “styles” of fake so it learns a broad concept of fakeness.

----------------------------------------------------------------------------------

Final notes:

Heygen is a single method that has only around 75 videos.

Veo3 is also a single method that has around 150 videos. Compare that to over 1,000 videos per other methods in DF40 and in FaceForensics, and you get a better picture.
