#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified script to define and validate a complete deepfake detection strategy.

This script performs the following phases:
1.  Foundation & Baseline Establishment: Loads OOF scores, establishes a baseline
    dataset by excluding specified methods (e.g., faceforensics++), and calculates
    baseline performance metrics.
2.  Data-Driven Bottleneck Identification: Automatically identifies and ranks
    "bottleneck" fake methods whose poor performance disproportionately harms
    the overall recall.
3.  "Supported & Uncertain" Strategy Definition: Creates a final "Supported"
    dataset by removing the top bottlenecks and calculates the T_low and T_high
    thresholds for a three-way (REAL, FAKE, UNCERTAIN) classification system
    that flags a small percentage of videos as uncertain.
4.  Final Validation & Comprehensive Reporting: Applies the full strategy to
    the dataset and generates a detailed report, including an executive summary
    and per-method performance breakdowns.
"""
import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.text import Text

# --- Configuration ---
# --- You can easily modify these parameters to test different strategies ---

# Path to the out-of-fold (OOF) scores generated by the fusion script.
# The script will try to load a .parquet file first for speed, then fall back to .csv.
OOF_SCORES_PATH = "./out_full_v3/fusion_oof_scores"

# The column name for the fusion model scores you want to analyze.
# From your logs, 'noisy_or' and 'stacked_logit_nonneg' are the best.
# 'noisy_or' is often a robust and simple choice.
FUSION_SCORE_COLUMN = 'noisy_or'

# Methods to completely exclude from all analysis.
# 'faceforensics++' is a common choice as it's an older, saturated benchmark.
INITIAL_EXCLUSIONS = ['faceforensics++', 'hey_gen', 'veo3_creations', 'fake_social_12_09', 'real_social_12_09']

# The number of top bottleneck methods to identify and exclude to form the
# "Supported Methods" dataset.
NUM_BOTTLENECKS_TO_REMOVE = 3

# The target percentage of videos to be classified as "UNCERTAIN".
# For example, 0.02 means the script will define a score band that captures
# the most ambiguous 2% of videos in the "Supported" dataset.
UNCERTAIN_PERCENTILE = 0.02

# A list of methods that MUST be included in the "Supported" set.
# These methods will never be identified as bottlenecks, even if their
# performance is low.
PROTECTED_METHODS = ['dfdc_fake', 'phase1_fake', 'phase1_real']


# --- End of Configuration ---


def youden_threshold(y_true: np.ndarray, y_score: np.ndarray) -> float:
    """Calculates the Youden's J statistic and the optimal threshold."""
    fpr, tpr, thr = roc_curve(y_true, y_score)
    j_scores = tpr - fpr
    j_opt_idx = np.argmax(j_scores)
    return float(thr[j_opt_idx])


def load_data(path: str, console: Console) -> pd.DataFrame:
    """Loads data from Parquet or CSV."""
    try:
        df = pd.read_parquet(f"{path}.parquet")
        console.print(f"[green]✔ Loaded data from [bold]{path}.parquet[/bold][/green]")
        return df
    except FileNotFoundError:
        try:
            df = pd.read_csv(f"{path}.csv")
            console.print(f"[green]✔ Loaded data from [bold]{path}.csv[/bold][/green]")
            return df
        except FileNotFoundError:
            console.print(
                f"[bold red]Error: Could not find data at '{path}.parquet' or '{path}.csv'.[/bold red]")
            exit()


def phase_1_establish_baseline(df: pd.DataFrame, console: Console) -> tuple:
    """Establishes the baseline dataset and calculates its core performance."""
    console.rule("[bold cyan]Phase 1: Foundation & Baseline Establishment[/bold cyan]")

    console.print(f"Original dataset size: {len(df):,} videos")
    console.print(f"Using fusion scores from column: [bold magenta]'{FUSION_SCORE_COLUMN}'[/bold magenta]")

    baseline_df = df[~df['method'].isin(INITIAL_EXCLUSIONS)].copy()
    console.print(f"Excluding methods {INITIAL_EXCLUSIONS}. Baseline dataset size: {len(baseline_df):,} videos.")

    reals_baseline = baseline_df[baseline_df['label'] == 0]
    fakes_baseline = baseline_df[baseline_df['label'] == 1]

    # Calculate baseline threshold for <1% FPR
    # This means 99% of real videos should have a score BELOW this threshold.
    threshold_1pct_fpr = reals_baseline[FUSION_SCORE_COLUMN].quantile(0.99)
    recall_at_1pct_fpr = (fakes_baseline[FUSION_SCORE_COLUMN] >= threshold_1pct_fpr).mean()

    # Calculate baseline threshold for Youden's J (balanced point)
    threshold_youden = youden_threshold(baseline_df['label'], baseline_df[FUSION_SCORE_COLUMN])
    recall_at_youden = (fakes_baseline[FUSION_SCORE_COLUMN] >= threshold_youden).mean()

    console.print("\n[bold]Baseline Performance Metrics:[/bold]")
    console.print(f"  - Threshold for <1% FPR: [cyan]{threshold_1pct_fpr:.6f}[/cyan]")
    console.print(f"  - Overall Recall at <1% FPR: [bold green]{recall_at_1pct_fpr:.2%}[/bold green]")
    console.print(f"  - Threshold for Youden (balanced): [cyan]{threshold_youden:.6f}[/cyan]")
    console.print(f"  - Overall Recall at Youden: [bold green]{recall_at_youden:.2%}[/bold green]\n")

    return baseline_df, recall_at_youden, threshold_youden


def phase_2_identify_bottlenecks(baseline_df: pd.DataFrame, baseline_recall: float, baseline_threshold: float,
                                 console: Console) -> list:
    """Identifies and ranks fake methods that are performance bottlenecks."""
    console.rule("[bold cyan]Phase 2: Data-Driven Bottleneck Identification[/bold cyan]")

    fakes_baseline = baseline_df[baseline_df['label'] == 1]
    unique_fake_methods = fakes_baseline['method'].unique()

    impact_results = []

    for method in unique_fake_methods:
        fakes_subset = fakes_baseline[fakes_baseline['method'] != method]

        # Calculate recall on the subset using the FIXED baseline Youden threshold
        recall_after_removal = (fakes_subset[FUSION_SCORE_COLUMN] >= baseline_threshold).mean()

        # Performance of the method being considered for removal
        method_tpr = (fakes_baseline[fakes_baseline['method'] == method][
                          FUSION_SCORE_COLUMN] >= baseline_threshold).mean()

        impact_results.append({
            'method': method,
            'recall_gain': recall_after_removal - baseline_recall,
            'individual_tpr': method_tpr,
            'count': len(fakes_baseline[fakes_baseline['method'] == method])
        })

    results_df = pd.DataFrame(impact_results).sort_values(by='recall_gain', ascending=False)

    table = Table(title=f"Top {NUM_BOTTLENECKS_TO_REMOVE} Performance Bottlenecks (Ranked by Impact on Recall)")
    table.add_column("Rank", style="bold yellow")
    table.add_column("Method Removed", style="bold magenta")
    table.add_column("Recall Gain", style="green", justify="right")
    table.add_column("Individual TPR", style="cyan", justify="right")
    table.add_column("Video Count", style="dim", justify="right")

    top_bottlenecks = results_df.head(NUM_BOTTLENECKS_TO_REMOVE)
    for i, row in enumerate(top_bottlenecks.itertuples(), 1):
        table.add_row(
            f"#{i}",
            row.method,
            f"+{row.recall_gain:.2%}",
            f"{row.individual_tpr:.2%}",
            f"{row.count:,}"
        )

    console.print(table)
    console.print(
        f"[italic]Interpretation: Removing '[bold magenta]{top_bottlenecks.iloc[0]['method']}[/bold magenta]' would increase the overall fake recall by [green]{top_bottlenecks.iloc[0]['recall_gain']:.2%}[/green].[/italic]\n")

    all_bottlenecks = results_df.head(NUM_BOTTLENECKS_TO_REMOVE)['method'].tolist()
    bottleneck_methods = [m for m in all_bottlenecks if m not in PROTECTED_METHODS]

    if len(all_bottlenecks) != len(bottleneck_methods):
        protected_found = set(all_bottlenecks) & set(PROTECTED_METHODS)
        console.print(
            f"[yellow]Note:[/yellow] The following methods were identified as top bottlenecks but are protected and will [bold]not[/bold] be excluded: [bold magenta]{list(protected_found)}[/bold magenta]\n")

    return bottleneck_methods
    return bottleneck_methods


def phase_3_define_strategy(baseline_df: pd.DataFrame, bottleneck_methods: list, console: Console) -> tuple:
    """Defines the final strategy: the supported dataset and uncertain thresholds."""
    console.rule("[bold cyan]Phase 3: Defining the 'Supported & Uncertain' Strategy[/bold cyan]")

    unsupported_methods = INITIAL_EXCLUSIONS + bottleneck_methods
    supported_df = baseline_df[~baseline_df['method'].isin(bottleneck_methods)].copy()

    console.print(f"Identified {len(bottleneck_methods)} bottlenecks to treat as 'Unsupported': {bottleneck_methods}")
    console.print(f"Final 'Supported Methods' dataset size: {len(supported_df):,} videos.")

    # Find the center point for the uncertain band using the "Supported" data
    center_threshold = youden_threshold(supported_df['label'], supported_df[FUSION_SCORE_COLUMN])
    console.print(f"\nCenter of uncertain band (Youden on supported set): [cyan]{center_threshold:.6f}[/cyan]")

    # Define the uncertain band width
    supported_df['distance_from_center'] = np.abs(supported_df[FUSION_SCORE_COLUMN] - center_threshold)
    band_width = supported_df['distance_from_center'].quantile(UNCERTAIN_PERCENTILE)

    T_low = center_threshold - band_width
    T_high = center_threshold + band_width

    # Verify the percentage of uncertain videos
    percent_uncertain = (supported_df[FUSION_SCORE_COLUMN].between(T_low, T_high)).mean()

    strategy_panel = Panel(
        Text(f"T_low: {T_low:.6f}\nT_high: {T_high:.6f}", justify="center"),
        title="[bold]Final Decision Thresholds[/bold]",
        border_style="green",
        expand=False
    )
    console.print(strategy_panel)
    console.print(
        f"These thresholds classify [yellow]{percent_uncertain:.2%}[/yellow] of 'Supported' videos as UNCERTAIN (target was {UNCERTAIN_PERCENTILE:.2%}).\n")

    return T_low, T_high, unsupported_methods


def phase_4_validate_and_report(df: pd.DataFrame, T_low: float, T_high: float, unsupported_methods: list,
                                console: Console):
    """Applies the final strategy and generates comprehensive reports."""
    console.rule("[bold cyan]Phase 4: Final Validation & Comprehensive Reporting[/bold cyan]")

    # Use the full original dataset for the final report
    report_df = df.copy()

    # --- Add classification and category columns for easy grouping ---
    def classify_video(score: float) -> str:
        if score < T_low:
            return "REAL (Miss)"
        elif score > T_high:
            return "FAKE (Caught)"
        else:
            return "UNCERTAIN"

    report_df['classification'] = report_df[FUSION_SCORE_COLUMN].apply(classify_video)

    def get_category(row):
        if row['method'] in INITIAL_EXCLUSIONS:
            return "Excluded"
        elif row['label'] == 0:
            return "Real Videos"
        elif row['method'] in unsupported_methods:
            return "Unsupported Fakes"
        else:
            return "Supported Fakes"

    report_df['category'] = report_df.apply(get_category, axis=1)

    # Filter out the initially excluded methods for reporting
    report_df = report_df[report_df['category'] != "Excluded"].copy()

    # --- Calculations for the Executive Summary Table ---
    grouped = report_df.groupby('category')['classification'].value_counts(normalize=True).unstack(fill_value=0)
    df_fakes_only = report_df[report_df['label'] == 1]
    overall_fake_perf = df_fakes_only['classification'].value_counts(normalize=True)

    # --- Create and print the rich tables ---
    # Executive Summary Table
    exec_table = Table(title="[bold]Executive Performance Summary[/bold]", style="cyan", title_justify="left",
                       show_lines=True)
    exec_table.add_column("Category", style="bold magenta", max_width=50)
    exec_table.add_column("TPR (Caught as FAKE)", justify="right")
    exec_table.add_column("% UNCERTAIN", justify="right")
    exec_table.add_column("% Miss (Classified as REAL)", justify="right")

    def add_fake_row(table, name, data):
        table.add_row(
            name,
            f"[bold green]{data.get('FAKE (Caught)', 0):.2%}[/bold green]",
            f"[yellow]{data.get('UNCERTAIN', 0):.2%}[/yellow]",
            f"[red]{data.get('REAL (Miss)', 0):.2%}[/red]"
        )

    add_fake_row(exec_table, "Performance on Supported Fakes", grouped.loc['Supported Fakes'])
    add_fake_row(exec_table, "Incidental Performance on Unsupported Fakes", grouped.loc['Unsupported Fakes'])
    exec_table.add_row("Overall Performance (All Fakes Combined)",
                       f"[bold]{overall_fake_perf.get('FAKE (Caught)', 0):.2%}[/bold]",
                       f"{overall_fake_perf.get('UNCERTAIN', 0):.2%}",
                       f"{overall_fake_perf.get('REAL (Miss)', 0):.2%}")

    # Calculate FPR on 'Certain' real videos
    reals_df = report_df[report_df['category'] == 'Real Videos']
    reals_certain = reals_df[reals_df['classification'] != 'UNCERTAIN']
    fpr = (reals_certain['classification'] == 'FAKE (Caught)').mean() if not reals_certain.empty else 0

    exec_table.add_row(
        "Performance on Real Videos",
        f"[bold red]FPR (on certain): {fpr:.3%}[/bold red]",
        f"[yellow]{grouped.loc['Real Videos'].get('UNCERTAIN', 0):.2%}[/yellow]",
        f"[green]{grouped.loc['Real Videos'].get('REAL (Miss)', 0):.2%} (Correct)[/green]"
    )
    console.print(exec_table)

    # --- Per-Method Breakdown Table ---
    per_method_table = Table(title="[bold]Per-Method Breakdown (All Fake Methods)[/bold]", style="cyan",
                             title_justify="left")
    per_method_table.add_column("Method", style="bold magenta")
    per_method_table.add_column("Category", style="yellow")
    per_method_table.add_column("TPR (Caught)", justify="right")
    per_method_table.add_column("% Uncertain", justify="right")
    per_method_table.add_column("% Missed", justify="right")
    per_method_table.add_column("Count", justify="right", style="dim")

    method_grouped = df_fakes_only.groupby(['method', 'category'])['classification'].value_counts(
        normalize=True).unstack(fill_value=0)
    method_counts = df_fakes_only['method'].value_counts()

    # Sort for consistent display
    sorted_methods = sorted(method_counts.index)

    # Add supported methods first
    for method in sorted_methods:
        category = "Supported" if method not in unsupported_methods else "Unsupported"
        if category == "Unsupported":
            continue
        try:
            data = method_grouped.loc[(method, 'Supported Fakes')]
            per_method_table.add_row(
                method, category, f"{data.get('FAKE (Caught)', 0):.1%}",
                f"{data.get('UNCERTAIN', 0):.1%}", f"{data.get('REAL (Miss)', 0):.1%}",
                f"{method_counts[method]:,}"
            )
        except KeyError:
            pass  # Method might have been fully excluded

    per_method_table.add_section()

    # Add unsupported methods
    for method in sorted_methods:
        category = "Supported" if method not in unsupported_methods else "Unsupported"
        if category == "Supported":
            continue
        try:
            data = method_grouped.loc[(method, 'Unsupported Fakes')]
            per_method_table.add_row(
                method, category, f"{data.get('FAKE (Caught)', 0):.1%}",
                f"{data.get('UNCERTAIN', 0):.1%}", f"{data.get('REAL (Miss)', 0):.1%}",
                f"{method_counts[method]:,}"
            )
        except KeyError:
            pass

    console.print(per_method_table)


def main():
    """Main execution function."""
    console = Console()
    console.rule("[bold]Deepfake Detection Strategy Optimizer[/bold]")

    # --- Execute Pipeline ---
    df_full = load_data(OOF_SCORES_PATH, console)

    baseline_df, recall, threshold = phase_1_establish_baseline(df_full, console)

    bottleneck_methods = phase_2_identify_bottlenecks(baseline_df, recall, threshold, console)

    T_low, T_high, unsupported_methods = phase_3_define_strategy(baseline_df, bottleneck_methods, console)

    phase_4_validate_and_report(df_full, T_low, T_high, unsupported_methods, console)

    console.rule("[bold green]Strategy Definition Complete[/bold green]")


if __name__ == "__main__":
    main()
